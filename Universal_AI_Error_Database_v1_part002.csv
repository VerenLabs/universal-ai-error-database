ID,Date Logged,Category,Subcategory,User Prompt Summary,Model Output Summary,Expanded Description,Root Cause (Technical),AI Model Type,Architecture Scope,Model Layer Impact,Confidence Score,Confidence Context,Severity,Source Reference,Resolution / Fix (Expanded),Verification Status,Replicability,Detection Method,Validation Status,Re-occurrence Likelihood,Dataset Link
026,2025-10-16,Context,Context Window Loss,Summarise the first three paragraphs of a 25k-word policy PDF.,Blended early sections with conclusions; omitted definitions from the preface.,"The input exceeded the effective context window, so the earliest paragraphs were truncated by the tokenizer before attention passed to the model. The model then hallucinated bridging statements to connect mid-document sections to the conclusion. This yields a plausible summary that is unfaithful to the user’s request.",Context overflow without hierarchical chunking; lack of retrieval anchors to preserve early sections.,GPT-4,Transformer LLM,Context layer,0.63,Evaluator rating,High,ARC Long-Context Eval 2024,Use hierarchical summarisation: chunk → local summaries → global synthesis. Pin retrieval anchors (section headings) and warn when truncation is detected; require citations (page numbers) in the output.,True,High,Benchmark evaluation,Validated,0.46,LongDoc Eval v2
027,2025-10-16,Tool Use,Search Tool Misfire,Find the official date and organiser for the AI Safety Summit (UK).,Returned the AI Safety Forum (US) with the wrong organiser and year.,"The tool was invoked correctly but the reranker overweighted recency and domain authority, preferring a near-match entity. Without entity disambiguation between similarly named events, the snippet selection drifted.",Reranker bias toward recency; missing named-entity disambiguation; weak exact-title matching.,Claude 3 Opus,Transformer LLM,Retrieval layer,0.69,Evaluator rating,Medium,Anthropic Tool-Use Study 2024,Add a disambiguation gate that checks title tokens and organiser names against the query. Penalise snippets with partial name overlap and require an explicit quote from the target site.,True,High,Benchmark evaluation,Validated,0.40,ToolEval v1
028,2025-10-16,Multimodal,Image Misclassification,Identify object in photo: yellow banana on a laptop keyboard.,Predicted 'yellow computer mouse' with high confidence.,"Vision encoder attention focused on colour and centroid, not curvature and texture; low edge contrast blurred the banana’s silhouette. The language head reinforced the error by preferring a frequent office object that fits the colour prior.",Feature weighting imbalance in vision encoder; insufficient cross-modal agreement checks.,Gemini 1.5 Pro,Multimodal Transformer,Perception layer,0.74,Evaluator rating,Medium,Google Multimodal Notes 2024,"Introduce cross-modal verification: require agreement between shape, texture, and colour descriptors. Back off to uncertainty if modalities disagree; suggest alternative candidates.",True,High,Red team probe,Mitigated,0.48,ImageEval v1
029,2025-10-16,Context,Pronoun Coreference Failure,"In a story with Alice, Beth, and Cara, resolve the final 'she'.",Linked 'she' to the wrong character (Beth instead of Cara).,"Coreference collapsed to local proximity rather than narrative role. Without persistent entity embeddings and mention tracking, the model flips antecedents in long passages with multiple female entities.",Missing global entity memory; proximity bias in pronoun resolution.,LLaMA 3 70B,Transformer LLM,Reasoning layer,0.66,Evaluator rating,Medium,ARC Narrative Eval 2024,Maintain an entity graph with stable IDs across turns and paragraphs. Run a post-generation pass that validates pronouns against the most recent unambiguous antecedent.,True,High,Benchmark evaluation,Validated,0.44,CorefEval v1
030,2025-10-16,Interface,Streaming Cutoff,Explain step-by-step and stream tokens as they are generated.,Output stopped mid-sentence and did not resume.,"The server-side stream watchdog terminated the connection at a soft timeout. Because there was no resume token or completion status header, the client could not request continuation, leaving users with partial reasoning.",Interface timeout; no resumable cursor; missing completion status metadata.,GPT-4.1,Transformer LLM,Interface layer,0.79,Model self confidence,Medium,OpenAI Stream Incident 2024,Emit a resumable cursor every N tokens and include a completion-status flag. Client retries should automatically continue from the last confirmed token.,True,Medium,User feedback,Mitigated,0.38,InterfaceEval v1
031,2025-10-16,Tool Use,Calculator Routing Miss,Compute weighted average price with three weights and decimals.,Narrative math returned a slightly off result due to early rounding.,The tool-policy failed to route numeric work to a deterministic calculator because the problem looked 'simple'. Early rounding in prose caused cumulative drift.,Over-permissive tool policy; lack of precision guard; human-like rounding in-chain.,GPT-4,Transformer LLM,Reasoning layer,0.72,Evaluator rating,Medium,MSR Reliability Studies,Mandate calculator usage for multi-term arithmetic; carry full precision internally and round only for presentation. Validate prose result against calculator output and prefer the tool value.,True,High,Auto QA,Validated,0.41,CalcEval v2
032,2025-10-16,Context,Session Memory Leak,Continue yesterday’s plan to refactor module X and keep the same conventions.,Restarted with new conventions and contradicted prior naming.,"Cross-session state was not restored, so former decisions (naming, file layout) were not rehydrated. The model defaulted to generic conventions, causing inconsistencies that break diffs.",No cross-session memory binding; missing preference persistence and retrieval on resume.,Claude 3.5 Sonnet,Transformer LLM,Context layer,0.61,Evaluator rating,High,OpenAI Technical Report 2024,"Bind explicit preferences to a user/session memory slot. On 'continue' intents, retrieve and restate the last known decisions before proposing changes.",True,High,User feedback,Mitigated,0.52,DevFlow Eval v1
033,2025-10-16,Multimodal,Chart Axes Misread,Describe the trend in an uploaded line chart of revenue by month.,Claimed growth while the last three points decline.,"The model anchored on the left-side increase and ignored the right-tail downturn. Without a 'describe-then-infer' protocol, visual inferences over-weight early salience and neglect recency.",No recency weighting; missing describe-then-infer scaffold for visual reasoning.,GPT-4o,Multimodal Transformer,Reasoning layer,0.69,Evaluator rating,Medium,MSR Visual Eval 2024,"Enforce a two-stage routine: first enumerate visible peaks/valleys and last-3 data points, then infer the net trend. Add a rule that emphasises the final window.",True,High,Benchmark evaluation,Validated,0.43,VizEval v1
034,2025-10-16,Tool Use,Weather Tool Bypass,"What’s the weather tomorrow in Manchester, UK?",Gave a generic climate statement without forecast or tool citation.,"The decision boundary did not trigger the weather tool for a time-specific question. Fluency was rewarded over correctness, producing a plausible but useless answer.",Decision boundary too lax for time-volatile queries; missing tool hard-rule.,Mistral Large,Transformer LLM,Retrieval layer,0.70,Evaluator rating,High,OpenAI System Card 2024,Hard-route weather queries to the weather tool; include location disambiguation and timestamped source. Reject answers without a forecast block.,True,High,Auto QA,Mitigated,0.49,WeatherEval v1
035,2025-10-16,Context,Attachment Reference Loss,Use the attached PDF’s table to compute the totals.,Ignored the table and answered from memory.,"The attachment index was not incorporated into the retrieval step, so the model treated the prompt as free-form. Without explicit attachment grounding, answers drift to prior knowledge.",Missing attachment-to-retrieval binding; weak grounding signals.,GPT-4,Transformer LLM,Retrieval layer,0.64,Evaluator rating,High,HELM Notes 2024,"Construct an attachment index on upload and force top-k retrieval from that index when queries mention 'attached', 'this file', or file names. Include page/table refs in the answer.",True,High,Manual review,Mitigated,0.46,DocGround Eval v1
036,2025-10-16,Multimodal,OCR Misread,Read the serial number from a blurry product label photo.,Output had 'B' where the label shows '8'.,"The OCR head over-weighted character priors under blur, turning an '8' into a 'B'. Without uncertainty calibration or multi-pass sharpening, the wrong token was emitted with high confidence.",Vision-OCR confusion under blur; no uncertainty threshold for ambiguous glyphs.,Gemini 1.5 Flash,Multimodal Transformer,Perception layer,0.73,Evaluator rating,Medium,Google OCR Notes 2024,Apply deblurring and multi-crop OCR; require per-character confidence and flag ambiguous positions for human confirmation.,True,High,Red team probe,Mitigated,0.45,OCREval v2
037,2025-10-16,Tool Use,PDF Page Index Off-by-One,Screenshot page 3 of the PDF and summarise it.,Captured page 2 but labelled it as page 3.,"The screenshot tool uses zero-based indexing while the document UI shows one-based pages. Without index normalisation, the wrong page is captured and summarised.",Index base mismatch between tool and UI; no normalisation layer.,GPT-4,Transformer LLM,Retrieval layer,0.71,Evaluator rating,Medium,OpenAI PDF Tool Notes,Translate human page numbers to zero-based indices for the tool; echo both page labels in the answer to confirm alignment.,True,High,Auto QA,Validated,0.42,PDFEval v1
038,2025-10-16,Context,Name Recall Slip,Use the project name 'Aurora' consistently in this plan.,Switched to 'Aurore' mid-document.,"Token proximity to similar names caused a drift (Aurore, Aurora, Aurelia). Without anchored constraints, the decoder chose a high-probability near-miss that reads correctly but is wrong.",Nearest-neighbour token confusion; lack of hard constraints on named entities.,Qwen 2 72B,Transformer LLM,Reasoning layer,0.67,Evaluator rating,Low,CRFM Notes 2024,Add a named-entity constraint list for critical terms. Run a post-pass that scans for variants and normalises to the canonical form.,True,High,Manual review,Mitigated,0.36,NameGuard Eval v1
039,2025-10-16,Multimodal,Table-to-Text Error,"From the uploaded table image, report 'Total Cost' for Q3.",Reported the 'Subtotal' value instead of 'Total Cost'.,The model failed to map header semantics correctly after OCR; column alignment drifted due to merged cells. The language head then inferred the closest numeric near the requested label.,Header-semantic mapping failure post-OCR; column alignment drift.,GPT-4o,Multimodal Transformer,Perception layer,0.71,Evaluator rating,Medium,MSR TableQA 2024,Perform structure-aware table OCR; align headers to cells before extraction. Require explicit cell coordinates and show the cell path in the answer.,True,High,Benchmark evaluation,Validated,0.41,TableQAEval v1
040,2025-10-16,Tool Use,Time Tool Omission,What's the local time now in Tokyo if it's 3pm in London?,Answered without converting time zones; returned '3pm'.,"The time tool was not invoked even though the query is explicitly zoned. Without timezone normalisation, the model mirrors the input and fails to convert.",Tool decision boundary miss; missing timezone normaliser.,GPT-3.5,Transformer LLM,Retrieval layer,0.62,Evaluator rating,Medium,OpenAI System Card 2024,Hard-route time queries to the time tool; echo both source and target zones and show the conversion calculation.,True,Medium,User feedback,Mitigated,0.51,TimeEval v1
041,2025-10-16,Context,Cross-Thread Contamination,Continue the SEO plan for the garden tools client only.,Mixed in details from a previous chat about electronics.,"Caching and embedding similarities caused retrieval of neighbouring thread chunks. The model blended entities due to weak session isolation, producing a hybrid plan.",Session isolation failure; high-similarity chunk bleed across indices.,Claude 3 Opus,Transformer LLM,Context layer,0.58,Evaluator rating,High,OpenAI Technical Report 2024,Isolate session indices strictly; add conversation IDs to retrieval filters and penalise cross-thread entities in re-ranking.,True,High,User feedback,Mitigated,0.55,SessionGuard Eval v1
042,2025-10-16,Multimodal,Incorrect Alt-Text,Generate alt-text for a photo of a ramp at a building entrance.,"Described 'stairs' instead of 'ramp', harming accessibility.",Angle and shadows made the ramp look like steps; the model defaulted to the most common access feature. Accessibility impact is high because assistive tech relies on accurate alt-text.,Failure to model perspective and slope; priors override rare accessibility features.,Gemini 1.5 Pro,Multimodal Transformer,Perception layer,0.70,Evaluator rating,High,Accessibility Eval 2024,"Add a slope-detection heuristic and require uncertainty hedging when features are occluded. Prefer neutral descriptors when unsure (e.g., 'inclined entrance').",True,High,Red team probe,Mitigated,0.47,AltText Eval v1
043,2025-10-16,Tool Use,FX Rate Staleness,"Convert 1,200 GBP to EUR at today's rate and cite the source.",Returned a plausible amount but no source and likely stale rate.,"The tool-policy allowed answering from memory for a volatile metric. Without live lookup and timestamp, users get misleading values.",Volatility not encoded in tool routing; missing cite-and-timestamp rule.,Mistral Large,Transformer LLM,Retrieval layer,0.70,Evaluator rating,High,Public Incident Summaries,"Force live FX lookup; include rate, timestamp, and source domain. If offline, provide a range and recommend checking an official source.",True,High,Auto QA,Mitigated,0.49,FXEval v2
044,2025-10-16,Context,Reference Link Drop,Use 'the link I shared above' to pull the spec sheet.,Ignored the link and quoted from memory.,The system failed to index hyperlinks as retrievable artifacts; link anchors were not bound to the retrieval policy. The model then hallucinated specs based on brand priors.,No hyperlink-to-retrieval binding; reliance on brand priors.,GPT-4,Transformer LLM,Retrieval layer,0.64,Evaluator rating,High,HELM Notes 2024,"Capture and index links; when a user references 'the link above', prioritise that URL in retrieval and echo it back before answering.",True,High,Manual review,Mitigated,0.46,LinkGround Eval v1
045,2025-10-16,Multimodal,Audio Transcription Overlap,Transcribe a 2-speaker meeting recording with timestamps.,Merged overlapping speech and attributed to one speaker.,"The diarisation model under-segmented speakers during overlap, and the language head smoothed conflicts into a single transcript. Loss of speaker turns harms action items and attribution.",Speaker diarisation failure under overlap; weak confidence gating between ASR and text.,GPT-4o,Multimodal Transformer,Perception layer,0.68,Evaluator rating,Medium,Meeting ASR Eval 2024,Introduce overlap-aware diarisation; attach per-segment confidence and keep uncertain segments unmerged. Offer a revision pass highlighting ambiguous spans.,True,High,Benchmark evaluation,Validated,0.42,ASREval v2
046,2025-10-16,Tool Use,Sports Tool Bypass,Did Arsenal win last night? Include final score and scorer list.,Provided a generic season summary without the match result.,"Despite clear cues for a live result, the decision policy favoured a fluent generic answer. Without mandatory sports-tool routing, answers lack scores and scorers.",Tool routing miss for live sports; generic-answer bias.,Qwen 2 72B,Transformer LLM,Retrieval layer,0.69,Evaluator rating,High,OpenAI System Card 2024,"Hard-route to sports tool for 'last night/today' patterns; require final scoreline, date, and scorer list with source.",True,High,Auto QA,Mitigated,0.47,SportsEval v1
047,2025-10-16,Context,Instruction Drift Over Turns,Keep bullet lists under 6 items and no emojis for this thread.,Later response produced 9 bullets and added emojis.,Long-turn dialogues degrade instruction adherence without persistent constraint reminders. The decoder reverted to default stylistic habits after several turns.,Constraint memory decay; missing turn-level rule reinforcement.,Claude 3.5 Sonnet,Transformer LLM,Formatter,0.72,Evaluator rating,Low,OpenAI UX Notes 2024,Attach live constraints to the session state and render them in the system preamble every N turns. Add a post-filter that trims bullets and strips emojis.,True,High,User feedback,Mitigated,0.34,StyleGuard Eval v1
048,2025-10-16,Multimodal,Figure-Caption Mismatch,Generate a caption that matches the uploaded figure (bar chart: 2022–2025).,Caption referenced 2021 and labelled the tallest bar as 2023.,"Year labels were misread due to small tick fonts; the decoder hallucinated a familiar year. Without a numeric verification pass, captions drift from the figure semantics.",Tick-OCR failure; lack of numeric verification; prior-year bias.,Gemini 1.5 Pro,Multimodal Transformer,Reasoning layer,0.71,Evaluator rating,Medium,MSR Visual Eval 2024,"Perform tick-aware OCR and validate numeric mentions against extracted labels. If confidence is low, output a neutral caption that avoids specific years.",True,High,Benchmark evaluation,Validated,0.41,CaptionEval v1
049,2025-10-16,Tool Use,PDF Table Extraction Failure,Extract the 'Net Profit' row from the attached PDF’s table 4 and sum Q1–Q4.,Summed the 'Gross Profit' row instead and missed Q4 due to page break.,The extractor lost header context across the page break and defaulted to the nearest numeric row. Table-structure awareness was insufficient for multi-page continuity.,Header carryover lost across pages; page-break-aware table parsing missing.,GPT-4,Transformer LLM,Retrieval layer,0.72,Evaluator rating,High,PDF Table Eval 2024,Use structure-aware PDF parsing that keeps header state across pages; require row ID and page references in the output for auditability.,True,High,Auto QA,Mitigated,0.45,PDFTabEval v1
050,2025-10-16,Context,Ambiguous Reference Handling,Answer: 'Is it open today?' after a prior link to a café and a museum.,Answered about the museum; the user meant the café.,Ambiguity across multiple prior entities was not resolved with a clarifying question. The model chose the most recently mentioned entity instead of confirming intent.,Ambiguity not detected; missing clarification policy under multi-entity context.,Mistral Large,Transformer LLM,Reasoning layer,0.65,Evaluator rating,Medium,HELM Notes 2024,"Detect multi-entity ambiguity and ask a 1-line clarifying question before answering. If forced to answer, present both entities with separate opening hours and sources.",True,High,User feedback,Mitigated,0.44,AmbigEval v1
